{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8965057-7769-49ca-aef9-0a35e9bad7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-23T10:33:11.856990] INFO - ✅ Successfully read the config file\n",
      "[2025-12-23T10:33:12.488039] INFO - No existing files for table suppliers\n",
      "[2025-12-23T10:33:12.488617] INFO - Latest watermark for suppliers: None\n",
      "[2025-12-23T10:33:13.293849] SUCCESS - ✅ Successfully extracted data from suppliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-23T10:33:14.955142] SUCCESS - ✅ JSON file successfully written to gs://retailer-datalake-project-1172025/landing/supplier-db/suppliers/suppliers_23122025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-23T10:33:31.274287] SUCCESS - ✅ Audit log updated for suppliers\n",
      "[2025-12-23T10:33:31.299030] INFO - No existing files for table product_suppliers\n",
      "[2025-12-23T10:33:32.007552] INFO - Latest watermark for product_suppliers: 1900-01-01 00:00:00\n",
      "[2025-12-23T10:33:32.043918] SUCCESS - ✅ Successfully extracted data from product_suppliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-23T10:33:35.416937] SUCCESS - ✅ JSON file successfully written to gs://retailer-datalake-project-1172025/landing/supplier-db/product_suppliers/product_suppliers_23122025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-23T10:33:45.371171] SUCCESS - ✅ Audit log updated for product_suppliers\n",
      "✅ Logs successfully saved to GCS at gs://retailer-datalake-project-1172025/temp/pipeline_logs/pipeline_log_20251223103345.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logs stored in BigQuery for future analysis\n",
      "✅ Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"supplierMySQLToLanding\").getOrCreate()\n",
    "\n",
    "# Google Cloud Storage (GCS) Configuration variables\n",
    "GCS_BUCKET = \"retailer-datalake-project-1172025\"\n",
    "LANDING_PATH = f\"gs://{GCS_BUCKET}/landing/supplier-db/\"\n",
    "ARCHIVE_PATH = f\"gs://{GCS_BUCKET}/landing/supplier-db/archive/\"\n",
    "CONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/supplier_config.csv\"\n",
    "\n",
    "# BigQuery Configuration\n",
    "BQ_PROJECT = \"expanded-league-477308-m5\"\n",
    "BQ_AUDIT_TABLE = f\"{BQ_PROJECT}.temp_dataset.audit_log\"\n",
    "BQ_LOG_TABLE = f\"{BQ_PROJECT}.temp_dataset.pipeline_logs\"\n",
    "BQ_TEMP_PATH = f\"{GCS_BUCKET}/temp/\"  \n",
    "\n",
    "# MySQL Configuration\n",
    "MYSQL_CONFIG = {\n",
    "\n",
    "    \"url\": \"jdbc:mysql://34.122.222.81:3306/supplierDB\"\n",
    "    \"?cloudSqlInstance=expanded-league-477308-m5:us-central1:supplier-mysql-db\"\n",
    "    \"useSSL=false\"\n",
    "    \"&allowPublicKeyRetrieval=true\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"Mysql@1245\", \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize GCS & BigQuery Clients\n",
    "storage_client = storage.Client()\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "# Logging Mechanism\n",
    "log_entries = []  # Stores logs before writing to GCS\n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "def log_event(event_type, message, table=None):\n",
    "    \"\"\"Log an event and store it in the log list\"\"\"\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"event_type\": event_type,\n",
    "        \"message\": message,\n",
    "        \"table\": table\n",
    "    }\n",
    "    log_entries.append(log_entry)\n",
    "    print(f\"[{log_entry['timestamp']}] {event_type} - {message}\")  # Print for visibility\n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "def save_logs_to_gcs():\n",
    "    \"\"\"Save logs to a JSON file and upload to GCS\"\"\"\n",
    "    log_filename = f\"pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n",
    "    log_filepath = f\"temp/pipeline_logs/{log_filename}\"  \n",
    "    \n",
    "    json_data = json.dumps(log_entries, indent=4)\n",
    "\n",
    "    # Get GCS bucket\n",
    "    bucket = storage_client.bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(log_filepath)\n",
    "    \n",
    "    # Upload JSON data as a file\n",
    "    blob.upload_from_string(json_data, content_type=\"application/json\")\n",
    "\n",
    "    print(f\"✅ Logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_filepath}\")\n",
    "    \n",
    "def save_logs_to_bigquery():\n",
    "    \"\"\"Save logs to BigQuery\"\"\"\n",
    "    if log_entries:\n",
    "        log_df = spark.createDataFrame(log_entries)\n",
    "        log_df.write.format(\"bigquery\") \\\n",
    "            .option(\"table\", BQ_LOG_TABLE) \\\n",
    "            .option(\"temporaryGcsBucket\", BQ_TEMP_PATH) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print(\"✅ Logs stored in BigQuery for future analysis\")\n",
    "\n",
    "\n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "# Function to Read Config File from GCS\n",
    "def read_config_file():\n",
    "    df = spark.read.csv(CONFIG_FILE_PATH, header=True)\n",
    "    log_event(\"INFO\", \"✅ Successfully read the config file\")\n",
    "    return df\n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "# Function to Move Existing Files to Archive\n",
    "def move_existing_files_to_archive(table):\n",
    "    blobs = list(storage_client.bucket(GCS_BUCKET).list_blobs(prefix=f\"landing/supplier-db/{table}/\"))\n",
    "    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n",
    "    \n",
    "    if not existing_files:\n",
    "        log_event(\"INFO\", f\"No existing files for table {table}\")\n",
    "        return\n",
    "    \n",
    "    for file in existing_files:\n",
    "        source_blob = storage_client.bucket(GCS_BUCKET).blob(file)\n",
    "        \n",
    "        # Extract Date from File Name (products_27032025.json)\n",
    "        date_part = file.split(\"_\")[-1].split(\".\")[0]\n",
    "        year, month, day = date_part[-4:], date_part[2:4], date_part[:2]\n",
    "        \n",
    "        # Move to Archive\n",
    "        archive_path = f\"landing/supplier-db/archive/{table}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n",
    "        destination_blob = storage_client.bucket(GCS_BUCKET).blob(archive_path)\n",
    "        \n",
    "        # Copy file to archive and delete original\n",
    "        storage_client.bucket(GCS_BUCKET).copy_blob(source_blob, storage_client.bucket(GCS_BUCKET), destination_blob.name)\n",
    "        source_blob.delete()\n",
    "        \n",
    "        log_event(\"INFO\", f\"✅ Moved {file} to {archive_path}\", table=table)    \n",
    "        \n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "\n",
    "# Function to Get Latest Watermark from BigQuery Audit Table\n",
    "def get_latest_watermark(table_name):\n",
    "    query = f\"\"\"\n",
    "        SELECT MAX(load_timestamp) AS latest_timestamp\n",
    "        FROM `{BQ_AUDIT_TABLE}`\n",
    "        WHERE tablename = '{table_name}'\n",
    "    \"\"\"\n",
    "    query_job = bq_client.query(query)\n",
    "    result = query_job.result()\n",
    "    for row in result:\n",
    "        return row.latest_timestamp if row.latest_timestamp else \"1900-01-01 00:00:00\"\n",
    "    return \"1900-01-01 00:00:00\"\n",
    "        \n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "\n",
    "# Function to Extract Data from MySQL and Save to GCS\n",
    "def extract_and_save_to_landing(table, load_type, watermark_col):\n",
    "    try:\n",
    "        # Get Latest Watermark\n",
    "        last_watermark = get_latest_watermark(table) if load_type.lower() == \"incremental\" else None\n",
    "        log_event(\"INFO\", f\"Latest watermark for {table}: {last_watermark}\", table=table)\n",
    "        \n",
    "        # Generate SQL Query\n",
    "        query = f\"(SELECT * FROM {table}) AS t\" if load_type.lower() == \"full load\" else \\\n",
    "                f\"(SELECT * FROM {table} WHERE {watermark_col} > '{last_watermark}') AS t\"\n",
    "        \n",
    "        # Read Data from MySQL\n",
    "        df = (spark.read\n",
    "                .format(\"jdbc\")\n",
    "                .option(\"url\", MYSQL_CONFIG[\"url\"])\n",
    "                .option(\"user\", MYSQL_CONFIG[\"user\"])\n",
    "                .option(\"password\", MYSQL_CONFIG[\"password\"])\n",
    "                .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n",
    "                .option(\"dbtable\", query)\n",
    "                .load())\n",
    "        log_event(\"SUCCESS\", f\"✅ Successfully extracted data from {table}\", table=table)\n",
    "        \n",
    "        # Convert Spark DataFrame to JSON\n",
    "        pandas_df = df.toPandas()\n",
    "        json_data = pandas_df.to_json(orient=\"records\", lines=True)\n",
    "        \n",
    "        # Generate File Path in GCS\n",
    "        today = datetime.datetime.today().strftime('%d%m%Y')\n",
    "        JSON_FILE_PATH = f\"landing/supplier-db/{table}/{table}_{today}.json\"\n",
    "        \n",
    "        # Upload JSON to GCS\n",
    "        bucket = storage_client.bucket(GCS_BUCKET)\n",
    "        blob = bucket.blob(JSON_FILE_PATH)\n",
    "        blob.upload_from_string(json_data, content_type=\"application/json\")\n",
    "\n",
    "        log_event(\"SUCCESS\", f\"✅ JSON file successfully written to gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n",
    "        \n",
    "        # Insert Audit Entry\n",
    "        audit_df = spark.createDataFrame([\n",
    "            (table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")], [\"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n",
    "\n",
    "        (audit_df.write.format(\"bigquery\")\n",
    "            .option(\"table\", BQ_AUDIT_TABLE)\n",
    "            .option(\"temporaryGcsBucket\", GCS_BUCKET)\n",
    "            .mode(\"append\")\n",
    "            .save())\n",
    "\n",
    "        log_event(\"SUCCESS\", f\"✅ Audit log updated for {table}\", table=table)\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_event(\"ERROR\", f\"Error processing {table}: {str(e)}\", table=table)\n",
    "        \n",
    "##---------------------------------------------------------------------------------------------------##\n",
    "\n",
    "# Main Execution\n",
    "config_df = read_config_file()\n",
    "\n",
    "for row in config_df.collect():\n",
    "    if row[\"is_active\"] == '1':\n",
    "        db, src, table, load_type, watermark, _, targetpath = row\n",
    "        move_existing_files_to_archive(table)\n",
    "        extract_and_save_to_landing(table, load_type, watermark)\n",
    "\n",
    "save_logs_to_gcs()\n",
    "save_logs_to_bigquery()       \n",
    "        \n",
    "print(\"✅ Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7780e5-86ee-4afd-bb75-f257876ee234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
