{"cells": [{"cell_type": "code", "execution_count": 3, "id": "e8965057-7769-49ca-aef9-0a35e9bad7d2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[2025-12-23T10:27:41.768572] INFO - \u2705 Successfully read the config file\n[2025-12-23T10:27:42.524478] INFO - \u2705 Moved landing/retailer-db/products/products_23122025.json to landing/retailer-db/archive/products/2025/12/23/products_23122025.json\n[2025-12-23T10:27:42.524600] INFO - Latest watermark for products: None\n[2025-12-23T10:27:42.547115] SUCCESS - \u2705 Successfully extracted data from products\n[2025-12-23T10:27:42.793359] SUCCESS - \u2705 JSON file successfully written to gs://retailer-datalake-project-1172025/landing/retailer-db/products/products_23122025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-12-23T10:27:49.543306] SUCCESS - \u2705 Audit log updated for products\n[2025-12-23T10:27:49.873825] INFO - \u2705 Moved landing/retailer-db/categories/categories_23122025.json to landing/retailer-db/archive/categories/2025/12/23/categories_23122025.json\n[2025-12-23T10:27:49.873979] INFO - Latest watermark for categories: None\n[2025-12-23T10:27:49.896327] SUCCESS - \u2705 Successfully extracted data from categories\n[2025-12-23T10:27:50.093939] SUCCESS - \u2705 JSON file successfully written to gs://retailer-datalake-project-1172025/landing/retailer-db/categories/categories_23122025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-12-23T10:28:01.067254] SUCCESS - \u2705 Audit log updated for categories\n[2025-12-23T10:28:01.429415] INFO - \u2705 Moved landing/retailer-db/customers/customers_23122025.json to landing/retailer-db/archive/customers/2025/12/23/customers_23122025.json\n[2025-12-23T10:28:02.084148] INFO - Latest watermark for customers: 1900-01-01 00:00:00\n[2025-12-23T10:28:02.105812] SUCCESS - \u2705 Successfully extracted data from customers\n[2025-12-23T10:28:02.327507] SUCCESS - \u2705 JSON file successfully written to gs://retailer-datalake-project-1172025/landing/retailer-db/customers/customers_23122025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-12-23T10:28:09.249374] SUCCESS - \u2705 Audit log updated for customers\n[2025-12-23T10:28:09.679735] INFO - \u2705 Moved landing/retailer-db/orders/orders_23122025.json to landing/retailer-db/archive/orders/2025/12/23/orders_23122025.json\n[2025-12-23T10:28:10.456353] INFO - Latest watermark for orders: 1900-01-01 00:00:00\n[2025-12-23T10:28:10.476822] SUCCESS - \u2705 Successfully extracted data from orders\n[2025-12-23T10:28:10.687881] SUCCESS - \u2705 JSON file successfully written to gs://retailer-datalake-project-1172025/landing/retailer-db/orders/orders_23122025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-12-23T10:28:15.959590] SUCCESS - \u2705 Audit log updated for orders\n[2025-12-23T10:28:16.323469] INFO - \u2705 Moved landing/retailer-db/order_items/order_items_23122025.json to landing/retailer-db/archive/order_items/2025/12/23/order_items_23122025.json\n[2025-12-23T10:28:16.955675] INFO - Latest watermark for order_items: 1900-01-01 00:00:00\n[2025-12-23T10:28:16.978551] SUCCESS - \u2705 Successfully extracted data from order_items\n[2025-12-23T10:28:17.249923] SUCCESS - \u2705 JSON file successfully written to gs://retailer-datalake-project-1172025/landing/retailer-db/order_items/order_items_23122025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-12-23T10:28:22.687035] SUCCESS - \u2705 Audit log updated for order_items\n\u2705 Logs successfully saved to GCS at gs://retailer-datalake-project-1172025/temp/pipeline_logs/pipeline_log_20251223102822.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 Logs stored in BigQuery for future analysis\n\u2705 Pipeline completed successfully!\n"}], "source": "from google.cloud import storage, bigquery\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nimport datetime\nimport json\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"RetailerMySQLToLanding\").getOrCreate()\n\n# Google Cloud Storage (GCS) Configuration variables\nGCS_BUCKET = \"retailer-datalake-project-1172025\"\nLANDING_PATH = f\"gs://{GCS_BUCKET}/landing/retailer-db/\"\nARCHIVE_PATH = f\"gs://{GCS_BUCKET}/landing/retailer-db/archive/\"\nCONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/retailer_config.csv\"\n\n# BigQuery Configuration\nBQ_PROJECT = \"expanded-league-477308-m5\"\nBQ_AUDIT_TABLE = f\"{BQ_PROJECT}.temp_dataset.audit_log\"\nBQ_LOG_TABLE = f\"{BQ_PROJECT}.temp_dataset.pipeline_logs\"\nBQ_TEMP_PATH = f\"{GCS_BUCKET}/temp/\"  \n\n# MySQL Configuration\nMYSQL_CONFIG = {\n\n    \"url\": \"jdbc:mysql://34.57.168.126:3306/retailerDB\"\n    \"?cloudSqlInstance=expanded-league-477308-m5:us-central1:retailer-mysql-db\"\n    \"useSSL=false\"\n    \"&allowPublicKeyRetrieval=true\",\n    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n    \"user\": \"myuser\",\n    \"password\": \"Mysql@1245\", \n}\n\n\n\n# Initialize GCS & BigQuery Clients\nstorage_client = storage.Client()\nbq_client = bigquery.Client()\n\n# Logging Mechanism\nlog_entries = []  # Stores logs before writing to GCS\n##---------------------------------------------------------------------------------------------------##\ndef log_event(event_type, message, table=None):\n    \"\"\"Log an event and store it in the log list\"\"\"\n    log_entry = {\n        \"timestamp\": datetime.datetime.now().isoformat(),\n        \"event_type\": event_type,\n        \"message\": message,\n        \"table\": table\n    }\n    log_entries.append(log_entry)\n    print(f\"[{log_entry['timestamp']}] {event_type} - {message}\")  # Print for visibility\n##---------------------------------------------------------------------------------------------------##\ndef save_logs_to_gcs():\n    \"\"\"Save logs to a JSON file and upload to GCS\"\"\"\n    log_filename = f\"pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n    log_filepath = f\"temp/pipeline_logs/{log_filename}\"  \n    \n    json_data = json.dumps(log_entries, indent=4)\n\n    # Get GCS bucket\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blob = bucket.blob(log_filepath)\n    \n    # Upload JSON data as a file\n    blob.upload_from_string(json_data, content_type=\"application/json\")\n\n    print(f\"\u2705 Logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_filepath}\")\n    \ndef save_logs_to_bigquery():\n    \"\"\"Save logs to BigQuery\"\"\"\n    if log_entries:\n        log_df = spark.createDataFrame(log_entries)\n        log_df.write.format(\"bigquery\") \\\n            .option(\"table\", BQ_LOG_TABLE) \\\n            .option(\"temporaryGcsBucket\", BQ_TEMP_PATH) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"\u2705 Logs stored in BigQuery for future analysis\")\n\n\n##---------------------------------------------------------------------------------------------------##\n# Function to Read Config File from GCS\ndef read_config_file():\n    df = spark.read.csv(CONFIG_FILE_PATH, header=True)\n    log_event(\"INFO\", \"\u2705 Successfully read the config file\")\n    return df\n##---------------------------------------------------------------------------------------------------##\n# Function to Move Existing Files to Archive\ndef move_existing_files_to_archive(table):\n    blobs = list(storage_client.bucket(GCS_BUCKET).list_blobs(prefix=f\"landing/retailer-db/{table}/\"))\n    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n    \n    if not existing_files:\n        log_event(\"INFO\", f\"No existing files for table {table}\")\n        return\n    \n    for file in existing_files:\n        source_blob = storage_client.bucket(GCS_BUCKET).blob(file)\n        \n        # Extract Date from File Name (products_27032025.json)\n        date_part = file.split(\"_\")[-1].split(\".\")[0]\n        year, month, day = date_part[-4:], date_part[2:4], date_part[:2]\n        \n        # Move to Archive\n        archive_path = f\"landing/retailer-db/archive/{table}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n        destination_blob = storage_client.bucket(GCS_BUCKET).blob(archive_path)\n        \n        # Copy file to archive and delete original\n        storage_client.bucket(GCS_BUCKET).copy_blob(source_blob, storage_client.bucket(GCS_BUCKET), destination_blob.name)\n        source_blob.delete()\n        \n        log_event(\"INFO\", f\"\u2705 Moved {file} to {archive_path}\", table=table)    \n        \n##---------------------------------------------------------------------------------------------------##\n\n# Function to Get Latest Watermark from BigQuery Audit Table\ndef get_latest_watermark(table_name):\n    query = f\"\"\"\n        SELECT MAX(load_timestamp) AS latest_timestamp\n        FROM `{BQ_AUDIT_TABLE}`\n        WHERE tablename = '{table_name}'\n    \"\"\"\n    query_job = bq_client.query(query)\n    result = query_job.result()\n    for row in result:\n        return row.latest_timestamp if row.latest_timestamp else \"1900-01-01 00:00:00\"\n    return \"1900-01-01 00:00:00\"\n        \n##---------------------------------------------------------------------------------------------------##\n\n# Function to Extract Data from MySQL and Save to GCS\ndef extract_and_save_to_landing(table, load_type, watermark_col):\n    try:\n        # Get Latest Watermark\n        last_watermark = get_latest_watermark(table) if load_type.lower() == \"incremental\" else None\n        log_event(\"INFO\", f\"Latest watermark for {table}: {last_watermark}\", table=table)\n        \n        # Generate SQL Query\n        query = f\"(SELECT * FROM {table}) AS t\" if load_type.lower() == \"full load\" else \\\n                f\"(SELECT * FROM {table} WHERE {watermark_col} > '{last_watermark}') AS t\"\n        \n        # Read Data from MySQL\n        df = (spark.read\n                .format(\"jdbc\")\n                .option(\"url\", MYSQL_CONFIG[\"url\"])\n                .option(\"user\", MYSQL_CONFIG[\"user\"])\n                .option(\"password\", MYSQL_CONFIG[\"password\"])\n                .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n                .option(\"dbtable\", query)\n                .load())\n        log_event(\"SUCCESS\", f\"\u2705 Successfully extracted data from {table}\", table=table)\n        \n        # Convert Spark DataFrame to JSON\n        pandas_df = df.toPandas()\n        json_data = pandas_df.to_json(orient=\"records\", lines=True)\n        \n        # Generate File Path in GCS\n        today = datetime.datetime.today().strftime('%d%m%Y')\n        JSON_FILE_PATH = f\"landing/retailer-db/{table}/{table}_{today}.json\"\n        \n        # Upload JSON to GCS\n        bucket = storage_client.bucket(GCS_BUCKET)\n        blob = bucket.blob(JSON_FILE_PATH)\n        blob.upload_from_string(json_data, content_type=\"application/json\")\n\n        log_event(\"SUCCESS\", f\"\u2705 JSON file successfully written to gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n        \n        # Insert Audit Entry\n        audit_df = spark.createDataFrame([\n            (table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")], [\"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n\n        (audit_df.write.format(\"bigquery\")\n            .option(\"table\", BQ_AUDIT_TABLE)\n            .option(\"temporaryGcsBucket\", GCS_BUCKET)\n            .mode(\"append\")\n            .save())\n\n        log_event(\"SUCCESS\", f\"\u2705 Audit log updated for {table}\", table=table)\n    \n    except Exception as e:\n        log_event(\"ERROR\", f\"Error processing {table}: {str(e)}\", table=table)\n        \n##---------------------------------------------------------------------------------------------------##\n\n# Main Execution\nconfig_df = read_config_file()\n\nfor row in config_df.collect():\n    if row[\"is_active\"] == '1':\n        db, src, table, load_type, watermark, _, targetpath = row\n        move_existing_files_to_archive(table)\n        extract_and_save_to_landing(table, load_type, watermark)\n\nsave_logs_to_gcs()\nsave_logs_to_bigquery()       \n        \nprint(\"\u2705 Pipeline completed successfully!\")"}, {"cell_type": "code", "execution_count": null, "id": "0e7780e5-86ee-4afd-bb75-f257876ee234", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}